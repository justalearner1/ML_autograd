{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision cudatoolkit=9.0 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 315062624.0\n",
      "1 195602096.0\n",
      "2 59176312.0\n",
      "3 29596544.0\n",
      "4 22440144.0\n",
      "5 17879084.0\n",
      "6 14704166.0\n",
      "7 12375760.0\n",
      "8 10603801.0\n",
      "9 9223374.0\n",
      "10 8118314.0\n",
      "11 7215680.5\n",
      "12 6467172.0\n",
      "13 5835994.5\n",
      "14 5301155.0\n",
      "15 4842074.5\n",
      "16 4444275.5\n",
      "17 4096908.5\n",
      "18 3791527.75\n",
      "19 3520929.75\n",
      "20 3279968.5\n",
      "21 3061494.0\n",
      "22 2866001.0\n",
      "23 2689866.0\n",
      "24 2530457.5\n",
      "25 2385163.5\n",
      "26 2252744.0\n",
      "27 2131373.75\n",
      "28 2019901.0\n",
      "29 1917172.0\n",
      "30 1822592.75\n",
      "31 1735161.5\n",
      "32 1654157.5\n",
      "33 1578768.5\n",
      "34 1508654.625\n",
      "35 1443159.625\n",
      "36 1381985.625\n",
      "37 1324792.75\n",
      "38 1271058.0\n",
      "39 1220738.375\n",
      "40 1173360.0\n",
      "41 1128788.0\n",
      "42 1086801.5\n",
      "43 1047202.0\n",
      "44 1009831.4375\n",
      "45 974491.875\n",
      "46 940727.3125\n",
      "47 908718.0\n",
      "48 878420.875\n",
      "49 849701.6875\n",
      "50 822387.875\n",
      "51 796424.75\n",
      "52 771701.25\n",
      "53 748153.8125\n",
      "54 725704.0\n",
      "55 704081.75\n",
      "56 683484.625\n",
      "57 663813.5625\n",
      "58 645018.5625\n",
      "59 626980.625\n",
      "60 609734.375\n",
      "61 593190.625\n",
      "62 577341.9375\n",
      "63 562134.875\n",
      "64 547547.5625\n",
      "65 533533.4375\n",
      "66 520052.71875\n",
      "67 507093.40625\n",
      "68 494610.78125\n",
      "69 482591.40625\n",
      "70 471023.8125\n",
      "71 459871.59375\n",
      "72 449112.375\n",
      "73 438731.65625\n",
      "74 428661.4375\n",
      "75 418951.8125\n",
      "76 409582.4375\n",
      "77 400508.0625\n",
      "78 391747.0\n",
      "79 383272.125\n",
      "80 375078.6875\n",
      "81 367152.125\n",
      "82 359434.875\n",
      "83 351972.8125\n",
      "84 344747.375\n",
      "85 337738.375\n",
      "86 330949.875\n",
      "87 324368.0625\n",
      "88 317978.96875\n",
      "89 311774.5\n",
      "90 305744.9375\n",
      "91 299897.1875\n",
      "92 294214.78125\n",
      "93 288687.25\n",
      "94 283317.15625\n",
      "95 278097.0625\n",
      "96 273020.59375\n",
      "97 268077.96875\n",
      "98 263268.875\n",
      "99 258585.578125\n",
      "100 254025.65625\n",
      "101 249583.6875\n",
      "102 245256.65625\n",
      "103 241039.421875\n",
      "104 236929.953125\n",
      "105 232929.828125\n",
      "106 229027.953125\n",
      "107 225218.46875\n",
      "108 221506.640625\n",
      "109 217881.171875\n",
      "110 214342.625\n",
      "111 210890.15625\n",
      "112 207522.734375\n",
      "113 204232.890625\n",
      "114 201018.9375\n",
      "115 197880.140625\n",
      "116 194813.109375\n",
      "117 191813.34375\n",
      "118 188883.03125\n",
      "119 186016.984375\n",
      "120 183215.1875\n",
      "121 180476.3125\n",
      "122 177794.8125\n",
      "123 175172.15625\n",
      "124 172607.5625\n",
      "125 170096.1875\n",
      "126 167641.3125\n",
      "127 165235.5625\n",
      "128 162879.953125\n",
      "129 160570.5\n",
      "130 158309.6875\n",
      "131 156096.09375\n",
      "132 153927.46875\n",
      "133 151803.015625\n",
      "134 149722.0625\n",
      "135 147682.75\n",
      "136 145684.3125\n",
      "137 143727.078125\n",
      "138 141810.484375\n",
      "139 139926.84375\n",
      "140 138082.25\n",
      "141 136270.453125\n",
      "142 134495.328125\n",
      "143 132752.1875\n",
      "144 131041.734375\n",
      "145 129363.9375\n",
      "146 127719.140625\n",
      "147 126105.171875\n",
      "148 124518.5625\n",
      "149 122961.296875\n",
      "150 121432.5078125\n",
      "151 119931.90625\n",
      "152 118457.890625\n",
      "153 117010.9921875\n",
      "154 115589.0859375\n",
      "155 114192.5\n",
      "156 112820.8515625\n",
      "157 111474.25\n",
      "158 110150.8046875\n",
      "159 108851.5078125\n",
      "160 107574.328125\n",
      "161 106318.796875\n",
      "162 105085.0\n",
      "163 103871.578125\n",
      "164 102678.40625\n",
      "165 101506.2421875\n",
      "166 100353.65625\n",
      "167 99221.1015625\n",
      "168 98106.0\n",
      "169 97009.4609375\n",
      "170 95919.953125\n",
      "171 94852.359375\n",
      "172 93803.9375\n",
      "173 92773.9921875\n",
      "174 91760.53125\n",
      "175 90764.2109375\n",
      "176 89784.25\n",
      "177 88819.9375\n",
      "178 87869.6875\n",
      "179 86935.140625\n",
      "180 86015.4921875\n",
      "181 85110.8671875\n",
      "182 84219.765625\n",
      "183 83343.765625\n",
      "184 82480.6328125\n",
      "185 81629.8046875\n",
      "186 80792.4765625\n",
      "187 79968.578125\n",
      "188 79157.390625\n",
      "189 78357.640625\n",
      "190 77569.3828125\n",
      "191 76793.078125\n",
      "192 76028.09375\n",
      "193 75274.34375\n",
      "194 74532.0\n",
      "195 73800.546875\n",
      "196 73079.515625\n",
      "197 72368.78125\n",
      "198 71668.359375\n",
      "199 70978.03125\n",
      "200 70297.9375\n",
      "201 69627.40625\n",
      "202 68966.3515625\n",
      "203 68315.0625\n",
      "204 67672.8203125\n",
      "205 67039.40625\n",
      "206 66415.34375\n",
      "207 65799.796875\n",
      "208 65193.140625\n",
      "209 64594.52734375\n",
      "210 64003.34765625\n",
      "211 63420.671875\n",
      "212 62845.9140625\n",
      "213 62278.77734375\n",
      "214 61719.80078125\n",
      "215 61168.109375\n",
      "216 60623.82421875\n",
      "217 60087.3046875\n",
      "218 59557.12109375\n",
      "219 59034.515625\n",
      "220 58518.4140625\n",
      "221 58009.09375\n",
      "222 57506.1328125\n",
      "223 57009.95703125\n",
      "224 56520.58203125\n",
      "225 56037.828125\n",
      "226 55560.8203125\n",
      "227 55090.12109375\n",
      "228 54625.62109375\n",
      "229 54166.98828125\n",
      "230 53713.79296875\n",
      "231 53266.37109375\n",
      "232 52824.75390625\n",
      "233 52388.70703125\n",
      "234 51958.1640625\n",
      "235 51533.1484375\n",
      "236 51113.21875\n",
      "237 50698.40234375\n",
      "238 50288.9609375\n",
      "239 49884.765625\n",
      "240 49485.28515625\n",
      "241 49090.90234375\n",
      "242 48700.98828125\n",
      "243 48316.12109375\n",
      "244 47935.859375\n",
      "245 47560.01953125\n",
      "246 47188.8828125\n",
      "247 46822.34765625\n",
      "248 46460.0390625\n",
      "249 46101.68359375\n",
      "250 45747.875\n",
      "251 45398.046875\n",
      "252 45052.0703125\n",
      "253 44710.24609375\n",
      "254 44371.11328125\n",
      "255 44036.4453125\n",
      "256 43705.82421875\n",
      "257 43379.37109375\n",
      "258 43056.7734375\n",
      "259 42737.77734375\n",
      "260 42422.2890625\n",
      "261 42110.6015625\n",
      "262 41802.83984375\n",
      "263 41498.3046875\n",
      "264 41197.390625\n",
      "265 40899.89453125\n",
      "266 40605.8125\n",
      "267 40315.171875\n",
      "268 40027.49609375\n",
      "269 39743.3125\n",
      "270 39462.2578125\n",
      "271 39184.14453125\n",
      "272 38909.26171875\n",
      "273 38637.578125\n",
      "274 38368.296875\n",
      "275 38102.08984375\n",
      "276 37838.91015625\n",
      "277 37578.55859375\n",
      "278 37320.98828125\n",
      "279 37066.47265625\n",
      "280 36814.6015625\n",
      "281 36565.3671875\n",
      "282 36318.89453125\n",
      "283 36074.84375\n",
      "284 35833.484375\n",
      "285 35594.765625\n",
      "286 35358.69140625\n",
      "287 35124.92578125\n",
      "288 34893.7421875\n",
      "289 34664.90625\n",
      "290 34438.59375\n",
      "291 34214.7265625\n",
      "292 33992.890625\n",
      "293 33773.41015625\n",
      "294 33556.2734375\n",
      "295 33341.41796875\n",
      "296 33128.70703125\n",
      "297 32918.19921875\n",
      "298 32709.96875\n",
      "299 32503.6640625\n",
      "300 32299.27734375\n",
      "301 32097.234375\n",
      "302 31897.3046875\n",
      "303 31699.37109375\n",
      "304 31503.4375\n",
      "305 31309.306640625\n",
      "306 31117.158203125\n",
      "307 30927.0234375\n",
      "308 30738.5703125\n",
      "309 30552.068359375\n",
      "310 30367.609375\n",
      "311 30184.82421875\n",
      "312 30003.779296875\n",
      "313 29824.4921875\n",
      "314 29647.16796875\n",
      "315 29471.447265625\n",
      "316 29297.37890625\n",
      "317 29124.962890625\n",
      "318 28954.255859375\n",
      "319 28785.2265625\n",
      "320 28617.90625\n",
      "321 28452.1171875\n",
      "322 28288.029296875\n",
      "323 28125.439453125\n",
      "324 27964.41015625\n",
      "325 27804.861328125\n",
      "326 27646.88671875\n",
      "327 27490.306640625\n",
      "328 27335.1484375\n",
      "329 27181.3984375\n",
      "330 27029.208984375\n",
      "331 26878.486328125\n",
      "332 26729.115234375\n",
      "333 26581.033203125\n",
      "334 26434.42578125\n",
      "335 26289.041015625\n",
      "336 26145.177734375\n",
      "337 26002.66015625\n",
      "338 25861.33203125\n",
      "339 25721.29296875\n",
      "340 25582.474609375\n",
      "341 25445.05859375\n",
      "342 25308.71484375\n",
      "343 25173.669921875\n",
      "344 25039.72265625\n",
      "345 24907.064453125\n",
      "346 24775.65625\n",
      "347 24645.40625\n",
      "348 24516.25390625\n",
      "349 24388.220703125\n",
      "350 24261.451171875\n",
      "351 24135.80078125\n",
      "352 24011.232421875\n",
      "353 23887.859375\n",
      "354 23765.529296875\n",
      "355 23643.966796875\n",
      "356 23523.40625\n",
      "357 23403.94140625\n",
      "358 23285.62109375\n",
      "359 23168.32421875\n",
      "360 23052.0078125\n",
      "361 22936.73828125\n",
      "362 22822.623046875\n",
      "363 22709.37109375\n",
      "364 22596.986328125\n",
      "365 22485.560546875\n",
      "366 22375.142578125\n",
      "367 22265.6171875\n",
      "368 22157.1171875\n",
      "369 22049.498046875\n",
      "370 21942.548828125\n",
      "371 21836.451171875\n",
      "372 21731.294921875\n",
      "373 21627.1328125\n",
      "374 21523.716796875\n",
      "375 21421.201171875\n",
      "376 21319.62109375\n",
      "377 21218.771484375\n",
      "378 21118.79296875\n",
      "379 21019.69140625\n",
      "380 20921.4921875\n",
      "381 20824.140625\n",
      "382 20727.501953125\n",
      "383 20631.583984375\n",
      "384 20536.51171875\n",
      "385 20442.123046875\n",
      "386 20348.5546875\n",
      "387 20255.712890625\n",
      "388 20163.619140625\n",
      "389 20072.353515625\n",
      "390 19981.818359375\n",
      "391 19891.8828125\n",
      "392 19802.7734375\n",
      "393 19714.369140625\n",
      "394 19626.607421875\n",
      "395 19539.53515625\n",
      "396 19453.10546875\n",
      "397 19367.388671875\n",
      "398 19282.443359375\n",
      "399 19198.05078125\n",
      "400 19114.314453125\n",
      "401 19031.248046875\n",
      "402 18948.796875\n",
      "403 18867.03515625\n",
      "404 18785.845703125\n",
      "405 18705.298828125\n",
      "406 18625.466796875\n",
      "407 18546.13671875\n",
      "408 18467.44921875\n",
      "409 18389.37109375\n",
      "410 18311.88671875\n",
      "411 18234.91796875\n",
      "412 18158.53125\n",
      "413 18082.7890625\n",
      "414 18007.572265625\n",
      "415 17932.98046875\n",
      "416 17858.900390625\n",
      "417 17785.390625\n",
      "418 17712.36328125\n",
      "419 17639.88671875\n",
      "420 17567.9453125\n",
      "421 17496.53125\n",
      "422 17425.609375\n",
      "423 17355.21484375\n",
      "424 17285.3515625\n",
      "425 17215.994140625\n",
      "426 17147.16015625\n",
      "427 17078.818359375\n",
      "428 17010.970703125\n",
      "429 16943.6484375\n",
      "430 16876.791015625\n",
      "431 16810.46875\n",
      "432 16744.556640625\n",
      "433 16679.115234375\n",
      "434 16614.142578125\n",
      "435 16549.60546875\n",
      "436 16485.54296875\n",
      "437 16421.919921875\n",
      "438 16358.771484375\n",
      "439 16296.0869140625\n",
      "440 16233.833984375\n",
      "441 16172.03515625\n",
      "442 16110.59765625\n",
      "443 16049.6015625\n",
      "444 15989.001953125\n",
      "445 15928.85546875\n",
      "446 15869.197265625\n",
      "447 15809.876953125\n",
      "448 15750.92578125\n",
      "449 15692.4189453125\n",
      "450 15634.4453125\n",
      "451 15576.685546875\n",
      "452 15519.3388671875\n",
      "453 15462.4013671875\n",
      "454 15405.8193359375\n",
      "455 15349.6279296875\n",
      "456 15293.8232421875\n",
      "457 15238.4365234375\n",
      "458 15183.4296875\n",
      "459 15128.76171875\n",
      "460 15074.5029296875\n",
      "461 15020.587890625\n",
      "462 14967.017578125\n",
      "463 14913.828125\n",
      "464 14860.958984375\n",
      "465 14808.431640625\n",
      "466 14756.259765625\n",
      "467 14704.451171875\n",
      "468 14652.990234375\n",
      "469 14601.8212890625\n",
      "470 14550.9912109375\n",
      "471 14500.50390625\n",
      "472 14450.330078125\n",
      "473 14400.462890625\n",
      "474 14350.9599609375\n",
      "475 14301.8681640625\n",
      "476 14253.0380859375\n",
      "477 14204.5029296875\n",
      "478 14156.26171875\n",
      "479 14108.31640625\n",
      "480 14060.662109375\n",
      "481 14013.330078125\n",
      "482 13966.2783203125\n",
      "483 13919.5703125\n",
      "484 13873.146484375\n",
      "485 13826.947265625\n",
      "486 13781.076171875\n",
      "487 13735.50390625\n",
      "488 13690.1845703125\n",
      "489 13645.1357421875\n",
      "490 13600.423828125\n",
      "491 13555.9833984375\n",
      "492 13511.783203125\n",
      "493 13467.8828125\n",
      "494 13424.220703125\n",
      "495 13380.865234375\n",
      "496 13337.7421875\n",
      "497 13294.90625\n",
      "498 13252.3232421875\n",
      "499 13210.001953125\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "#device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "device = torch.device('cpu') # Uncomment this to run on CPU\n",
    "\n",
    "# N is batch size;\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension;\n",
    "#D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 100\n",
    "\n",
    "# input data :batch of 64 times 1000 features\n",
    "# output data : 100 x 10 continues values (real scalars)\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)# generate normally distributed data of dim NxD_in, store it on device, requires_grad = False\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)# generate normally distributed data of dim NxD_out, store it on device, requires_grad = False\n",
    "\n",
    "#y_obs = y + 0.2*torch.randn(N,1)\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "# here the gradient will be computed for the variables that are related to the model learning something new,\n",
    "# i.e. the network weights in this case\n",
    "\n",
    "\n",
    "# WEIGHTS\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=torch.float, requires_grad=True)#generate normally distributed data of dim D_in x H, store it on device, requires_grad = True\n",
    "w2 = torch.randn(D_out, H, device=device, dtype=torch.float, requires_grad=True)#generate normally distributed data of dim H x D_out, store it on device, requires_grad = True\n",
    "\n",
    "# initialize loss value to a high number\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "   y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "   loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "   print(t, loss.item())\n",
    "\n",
    "   loss.backward()\n",
    "\n",
    "   with torch.no_grad():\n",
    "       w1 -= learning_rate * w1.grad\n",
    "       w2 -= learning_rate * w2.grad\n",
    "\n",
    "\n",
    "       w1.grad.zero_()\n",
    "       w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-983eca1edbd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# predict the values by multiplying x with weight matrix w1, then apply RELU activation and multiply the result by weight matrix w2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyReLU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "for iteration in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # predict the values by multiplying x with weight matrix w1, then apply RELU activation and multiply the result by weight matrix w2\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = F.relu(x.mm(w1)).mm(w2).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "# your code here ; the final prediction is given by matrix multiplying the data \n",
    "    #with the two set of weights, making the intermediate values non-negative (RELU activation function)\n",
    "\n",
    "    # calculate the mean squared error (MSE)\n",
    "    error = torch.nn.MSELoss # your code here\n",
    "\n",
    "    \n",
    "    #writer.add_scalar(tag=\"Last run\",scalar_value= error, global_step = iteration)\n",
    "    #writer.add_histogram(\"error distribution\",error)\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "   \n",
    "    #error.WHAT_FUNCTION_here ?\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # use w1.grad to update w2 according to the gradient descent formula\n",
    "        # use w2.grad to update w2 according to the gradient descent formula\n",
    "        # also use the learning_rate you set before!\n",
    "        w1 -= learning_rate * w1.grad # your code here\n",
    "        w2 -= learning_rate * w2.grad # your code here\n",
    "        \n",
    "    if iteration % 50 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" % (iteration, error))\n",
    "        w1_array.append(w1.cpu().detach().numpy())\n",
    "        w2_array.append(w2.cpu().detach().numpy())\n",
    "        errors.append(error.cpu().detach().numpy())\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    if loss_value < 1e-6:\n",
    "        print(\"Stopping gradient descent, algorithm converged, MSE loss is smaller than 1E-6\")\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer to question 2:\n",
    "Backpropagation is a supervisied learning algorithm used to train neural networks to find the least value of the error function. \n",
    "It uses the gradient descent method  and the weights with the least error function are considered \n",
    "to be the answer to the learning problem. It calculates the weights update in order to improve the network \n",
    "until it can perform its' task. \n",
    "Backpropagation requires the derivative of the activation functions to be known at design time.\n",
    "\n",
    "Automatic differentiation is a method that can automatically and analytically provide the derivatives to the training algorithm.\n",
    "It provides the derivatives for the backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
